{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e7e47-199f-46a8-9978-e6ec7033d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (299, 299, 3)\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "h5ad = pd.read_csv(\"/scratch/user/s4634945/STimage_Alen/h5ad_obs.csv\", index_col=0)\n",
    "h5ad['tile_tissue_mask_path'] = h5ad['tile_tissue_mask_path'].str.split('/', expand=True)[3].str[:-5]\n",
    "h5ad = h5ad.set_index(['tile_tissue_mask_path'])\n",
    "\n",
    "gene_exp = pd.read_csv(\"/scratch/user/s4634945/STimage_Alen/gene_exp_2.csv\", index_col=0)\n",
    "gene_exp.index = h5ad.index\n",
    "gene_exp['dataset'] = list(h5ad['library_id'])\n",
    "gene_exp = gene_exp[[\"COX6C\",\"dataset\"]]\n",
    "Y = gene_exp.loc[(gene_exp['dataset'] == \"block1\")]\n",
    "#Y = Y.iloc[:10,:]\n",
    "test_Y_FFPE = gene_exp.loc[(gene_exp['dataset'] == \"FFPE\")]\n",
    "#test_Y_FFPE = test_Y_FFPE.iloc[:10,:]\n",
    "\n",
    "\n",
    "xtrain = pd.read_csv(\"/scratch/user/s4634945/STimage_Alen/h5ad_obs.csv\", index_col=0)\n",
    "xtrain['tile_tissue_mask_path'] = xtrain['tile_tissue_mask_path'].str.split('/', expand=True)[3].str[:-5]\n",
    "xtrain = xtrain.loc[xtrain['library_id']=='block1']\n",
    "#xtrain = xtrain.iloc[:10,:]\n",
    "\n",
    "\n",
    "x_test = pd.read_csv(\"/scratch/user/s4634945/STimage_Alen/h5ad_obs.csv\", index_col=0)\n",
    "x_test['tile_tissue_mask_path'] = x_test['tile_tissue_mask_path'].str.split('/', expand=True)[3].str[:-5]\n",
    "X_test_FFPE =  x_test.loc[(x_test['library_id'] == \"FFPE\")]\n",
    "#X_test_FFPE = X_test_FFPE.iloc[:10,:]\n",
    "\n",
    "\n",
    "os.chdir('/scratch/user/s4634945/STimage_Alen/tiles/')\n",
    "x_train = np.array([np.array(Image.open(fname)) for fname in xtrain['tile_tissue_mask_path']+'.jpeg'])\n",
    "\n",
    "os.chdir('/scratch/user/s4634945/STimage_Alen/tiles/')\n",
    "X_test_FFPE = np.array([np.array(Image.open(fname)) for fname in X_test_FFPE ['tile_tissue_mask_path']+'.jpeg'])\n",
    "\n",
    "\n",
    "\n",
    "def normalise(set_name):\n",
    "    set_name = np.log(2*(set_name.iloc[:,:-1])+1)\n",
    "    set_name = preprocessing.StandardScaler().fit_transform(set_name)\n",
    "    set_name = pd.DataFrame(data=set_name)\n",
    "    set_name = set_name.apply(lambda x: [0 if y <= 0 else 1 for y in x])\n",
    "    set_name = set_name.astype('int64')\n",
    "    return set_name\n",
    "\n",
    "Y = normalise(Y)\n",
    "test_Y_FFPE = normalise(test_Y_FFPE)\n",
    "\n",
    "\n",
    "#print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "#print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "dropout_rate = 0.2\n",
    "image_size = 299  # We'll resize input images to this size.\n",
    "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
    "embedding_dim = 256  # Number of hidden units.\n",
    "num_blocks = 4  # Number of blocks.\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_classifier(blocks, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks.\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation)\n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay.\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=Y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "\n",
    "    v_, accuracy, top_5_accuracy = model.evaluate(X_test_FFPE, test_Y_FFPE)\n",
    "    model.save('/scratch/user/s4634945/STimage_Alen/pickle.h5')\n",
    "    return history\n",
    "\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "\n",
    "class gMLPLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(gMLPLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.channel_projection1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embedding_dim * 2),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
    "\n",
    "        self.spatial_projection = layers.Dense(\n",
    "            units=num_patches, bias_initializer=\"Ones\"\n",
    "        )\n",
    "\n",
    "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def spatial_gating_unit(self, x):\n",
    "        # Split x along the channel dimensions.\n",
    "        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].\n",
    "        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n",
    "        # Apply layer normalization.\n",
    "        v = self.normalize2(v)\n",
    "        # Apply spatial projection.\n",
    "        v_channels = tf.linalg.matrix_transpose(v)\n",
    "        v_projected = self.spatial_projection(v_channels)\n",
    "        v_projected = tf.linalg.matrix_transpose(v_projected)\n",
    "        # Apply element-wise multiplication.\n",
    "        return u * v_projected\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize1(inputs)\n",
    "        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n",
    "        x_projected = self.channel_projection1(x)\n",
    "        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n",
    "        x_spatial = self.spatial_gating_unit(x_projected)\n",
    "        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n",
    "        x_projected = self.channel_projection2(x_spatial)\n",
    "        # Add skip connection.\n",
    "        return x + x_projected\n",
    "\n",
    "\n",
    "gmlp_blocks = keras.Sequential(\n",
    "    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
    ")\n",
    "learning_rate = 0.003\n",
    "gmlp_classifier = build_classifier(gmlp_blocks)\n",
    "history = run_experiment(gmlp_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
